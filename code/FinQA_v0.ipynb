{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSR4CYpeE_MS",
    "outputId": "756bbf1e-2bdc-4567-8937-de0630a7bf54"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes datasets\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/FinQA/dataset\"\n",
    "\n",
    "import os, json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.json\")\n",
    "DEV_PATH = os.path.join(DATA_DIR, \"dev.json\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.json\")\n",
    "PRIVATE_TEST_PATH = os.path.join(DATA_DIR, \"private_test.json\")\n",
    "\n",
    "print(\"Check files:\")\n",
    "for p in [TRAIN_PATH, DEV_PATH, TEST_PATH, PRIVATE_TEST_PATH]:\n",
    "    print(p, os.path.exists(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIsXpcCjFTv_",
    "outputId": "7a232bbb-b2f5-4bdb-e513-4ed78dcc7b1b"
   },
   "outputs": [],
   "source": [
    "def load_json(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} samples from {path}\")\n",
    "    return data\n",
    "\n",
    "train_raw = load_json(TRAIN_PATH)\n",
    "dev_raw = load_json(DEV_PATH)\n",
    "test_raw = load_json(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0ZLgvKgFWRG",
    "outputId": "d4db517b-362f-49a7-c892-07dbfc50e15b"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def build_sft_record(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    mi = sample[\"qa\"][\"model_input\"]  # list of [id, text]\n",
    "\n",
    "    evidences = []\n",
    "    for i, (_id, text) in enumerate(mi):\n",
    "        t = \" \".join(text.split())\n",
    "        evidences.append(f\"[{i}] {t}\")\n",
    "    ev_block = \"\\n\".join(evidences)\n",
    "\n",
    "    gold = sample[\"qa\"][\"exe_ans\"]  # float\n",
    "\n",
    "    system = \"You are a financial numerical reasoning expert.\"\n",
    "    user = f\"\"\"Question:\n",
    "{q}\n",
    "\n",
    "Evidence:\n",
    "{ev_block}\n",
    "\n",
    "Please compute the final numeric answer.\n",
    "Always end your reply with:\n",
    "Final answer: <number>\"\"\"\n",
    "\n",
    "    assistant = f\"Final answer: {gold}\"\n",
    "\n",
    "    return {\n",
    "        \"system\": system,\n",
    "        \"user\": user,\n",
    "        \"assistant\": assistant,\n",
    "    }\n",
    "\n",
    "train_records = [build_sft_record(s) for s in train_raw]\n",
    "len(train_records), train_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDTAAW_jFYnF",
    "outputId": "f10c0a4d-d237-4b3c-e335-de733573a452"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def build_sft_record(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    mi = sample[\"qa\"][\"model_input\"]  # list of [id, text]\n",
    "\n",
    "    evidences = []\n",
    "    for i, (_id, text) in enumerate(mi):\n",
    "        t = \" \".join(text.split())\n",
    "        evidences.append(f\"[{i}] {t}\")\n",
    "    ev_block = \"\\n\".join(evidences)\n",
    "\n",
    "    gold = sample[\"qa\"][\"exe_ans\"]  # float\n",
    "\n",
    "    system = \"You are a financial numerical reasoning expert.\"\n",
    "    user = f\"\"\"Question:\n",
    "{q}\n",
    "\n",
    "Evidence:\n",
    "{ev_block}\n",
    "\n",
    "Please compute the final numeric answer.\n",
    "Always end your reply with:\n",
    "Final answer: <number>\"\"\"\n",
    "\n",
    "    assistant = f\"Final answer: {gold}\"\n",
    "\n",
    "    return {\n",
    "        \"system\": system,\n",
    "        \"user\": user,\n",
    "        \"assistant\": assistant,\n",
    "    }\n",
    "\n",
    "train_records = [build_sft_record(s) for s in train_raw]\n",
    "len(train_records), train_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "1825107efbb74e5583d027a646634ee4",
      "7e508bbe72a2469e9ecaad1a0598888c",
      "cb5b4b6b9a0945d1b6eb439fd797b813",
      "2acf6e9e3b324514886ddb60dee70e81",
      "77433fdf0efc4daa9c9f6d915a12b9e9",
      "d548943155c1473c8ebc35ad7f2943eb",
      "70a4086c2a944ed1ba30304376ac49c1",
      "45fc323c5323425f942186003100312f",
      "c397f4a54fd84cecbad886ae02b3e1cf",
      "24985de45961455fa85231f24dcdf696",
      "db4f413ade114ec78f4d7221d476fb0f"
     ]
    },
    "id": "cBCBzWB0Fo-i",
    "outputId": "853b0e40-e0b4-40a2-c008-136655c22c5d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME_MATH = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer_math = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_list(train_records)\n",
    "\n",
    "def add_text(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": example[\"user\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"assistant\"]},\n",
    "    ]\n",
    "    text = tokenizer_math.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_ds.map(add_text)\n",
    "print(train_ds[0][\"text\"][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "3536131c6e1c4014b8f74007d4445359",
      "a764744d38874c8e86d5b3de850b56a3",
      "24e46168a11e4f509a12cb6f324b88a0",
      "eeaa291a193a479f83d95521dc61e185",
      "9db1f3fbff7a4ea09718a30fd0d0e1df",
      "5e3336c34a1045d880cffbd7c33c0dc1",
      "50554901ad444564a8e60d02690c5a01",
      "0e0e7fe84c664c36b74dfe2eb05349cb",
      "48b7670e6b474c1fac812be5e94c1a90",
      "58b48c79f7f14573b33856b8b869fb0b",
      "630a988d1f424bc0b6cbc47dd976bcbd"
     ]
    },
    "id": "AtLduVOtHXmv",
    "outputId": "d7efcbfb-2f42-41ad-c330-be8bb6aed552"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_math_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model_math_base.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model_math = get_peft_model(model_math_base, lora_config)\n",
    "model_math.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "k8yoRmQLIFRz",
    "outputId": "ea3d6583-96c5-46e0-83b4-659c5f00c00c"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def data_collator(batch):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    enc = tokenizer_math(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
    "    return enc\n",
    "\n",
    "NUM_TRAIN = 2000\n",
    "train_ds_small = train_ds.select(range(min(NUM_TRAIN, len(train_ds))))\n",
    "print(\"Train subset size:\", len(train_ds_small))\n",
    "\n",
    "OUTPUT_DIR = \"/content/qwen2p5_7b_finqa_lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_math,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_small,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGrjLiD5IJVp",
    "outputId": "c5ab1d3d-116b-4607-fd4a-02c5150c6154"
   },
   "outputs": [],
   "source": [
    "LORA_SAVE_DIR = \"/content/qwen2p5_7b_finqa_lora\"\n",
    "model_math.save_pretrained(LORA_SAVE_DIR)\n",
    "tokenizer_math.save_pretrained(LORA_SAVE_DIR)\n",
    "\n",
    "print(\"LoRA adapter saved to:\", LORA_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "3cdf21d5393b4e389f8a98b3ccef75ec",
      "8a8f51dc93394c51b8d9ed846c2af57f",
      "6554e278d3334a0992875092981c6f9e",
      "c3af567ab59241d383860d1aae0594ec",
      "d580ba2a7b594b8f8a094291f96932e5",
      "e5d0a2fdfbf9402aa8a5fabb05104a3e",
      "edbc91df1d8a4a4686de8e128099f2db",
      "c3c7386cf89341058a1086f191890321",
      "c2fe01e3c3114f13bc5435288cb72575",
      "b1d320b7bd464b2ca33f39fa75dbe638",
      "9efec886fb6a49548e31876571e387e0",
      "19fb6b02eea340ceb5b9a6ca9acac632",
      "d3c2110d7df546699c1c69610b6a52da",
      "a57101a609d64a0f9ddc8a816652dcd7",
      "7c7d3146104f4bd69ef9e50e33f5f82e",
      "dab63d5e50004d408a274884e824a8e0",
      "b26ab63626aa459b85400bf85972f649",
      "34f36f63dfbc42fd979c63d83a204c66",
      "fbd7ec795f934409a1aa4502549a4213",
      "f9ee978aa35f487e81a66cefe895df36",
      "d621eee9e8ed41908c5ac799793a1d86",
      "98b62fb008c34b49981b458b48e0a159"
     ]
    },
    "id": "IBmEUVDiIMOA",
    "outputId": "1fec74dc-1b4c-4628-d27c-7be8383592a8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "RETRIEVER_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    RETRIEVER_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "retriever_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RETRIEVER_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "retriever_model.eval()\n",
    "\n",
    "gen_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "gen_model = PeftModel.from_pretrained(gen_base_model, LORA_SAVE_DIR)\n",
    "gen_model.eval()\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LORA_SAVE_DIR,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga72uwISMTv8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@torch.no_grad()\n",
    "def retriever_generate(prompt: str, max_new_tokens: int = 64) -> str:\n",
    "    inputs = retriever_tokenizer(prompt, return_tensors=\"pt\").to(retriever_model.device)\n",
    "    out_ids = retriever_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=retriever_tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = retriever_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if out.startswith(prompt):\n",
    "        out = out[len(prompt):]\n",
    "    return out.strip()\n",
    "\n",
    "def build_retriever_prompt(question: str, candidates: List[str]) -> str:\n",
    "    cand_lines = []\n",
    "    for i, text in enumerate(candidates):\n",
    "        t = \" \".join(text.split())\n",
    "        cand_lines.append(f\"[{i}] {t}\")\n",
    "    cand_block = \"\\n\".join(cand_lines)\n",
    "\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    You help solve financial numerical reasoning questions.\n",
    "\n",
    "    Given a question and a list of candidate evidence texts, select ALL evidence\n",
    "    sentences that are directly needed to compute the numeric answer.\n",
    "\n",
    "    Only output a comma-separated list of indices.\n",
    "    Do NOT output anything else.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Candidate evidence:\n",
    "    {cand_block}\n",
    "\n",
    "    Answer (indices only):\n",
    "    \"\"\").strip()\n",
    "    return prompt\n",
    "\n",
    "def parse_indices(s: str, max_idx: int) -> List[int]:\n",
    "    nums = re.findall(r\"\\d+\", s)\n",
    "    ids = []\n",
    "    for t in nums:\n",
    "        i = int(t)\n",
    "        if 0 <= i < max_idx:\n",
    "            ids.append(i)\n",
    "    return sorted(set(ids))\n",
    "\n",
    "def select_evidence(sample: Dict[str, Any]) -> List[str]:\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    mi = sample[\"qa\"][\"model_input\"]\n",
    "    candidates = [t for (_id, t) in mi]\n",
    "\n",
    "    prompt = build_retriever_prompt(q, candidates)\n",
    "    raw = retriever_generate(prompt)\n",
    "    idxs = parse_indices(raw, len(candidates))\n",
    "    if not idxs:\n",
    "        idxs = list(range(min(3, len(candidates))))  # fallback\n",
    "    selected = [candidates[i] for i in idxs]\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvbVi53aMZfe"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def qwen7b_answer_generate(\n",
    "    question: str,\n",
    "    evidence_texts: List[str],\n",
    "    tokenizer,\n",
    "    model,\n",
    "    max_new_tokens: int = 128,\n",
    ") -> str:\n",
    "    ev_block = \"\\n\".join(\"Evidence: \" + \" \".join(e.split()) for e in evidence_texts)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a financial numerical reasoning expert. \"\n",
    "        \"Given a question and evidence sentences, compute the final numeric answer. \"\n",
    "        \"You may show reasoning steps, but MUST end your response with: \"\n",
    "        \"'Final answer: <number>'. \"\n",
    "        \"Only output one final numeric value.\"\n",
    "    )\n",
    "\n",
    "    user_content = f\"\"\"Question:\n",
    "{question}\n",
    "\n",
    "Evidence:\n",
    "{ev_block}\n",
    "\n",
    "Please compute the final numeric answer.\n",
    "Always end with:\n",
    "Final answer: <number>\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    chat_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = chat_inputs\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    input_len = input_ids.shape[1]\n",
    "    out_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = out_ids[0, input_len:]\n",
    "    out = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return out.strip()\n",
    "\n",
    "def extract_final_number(output: str):\n",
    "    m = re.search(r\"Final answer\\s*:\\s*([-+]?\\d*\\.?\\d+)\", output, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", output)\n",
    "    if nums:\n",
    "        return float(nums[-1])\n",
    "    return None\n",
    "\n",
    "def generate_answer(sample: Dict[str, Any], selected_texts: List[str]):\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    raw = qwen7b_answer_generate(\n",
    "        question=q,\n",
    "        evidence_texts=selected_texts,\n",
    "        tokenizer=gen_tokenizer,\n",
    "        model=gen_model,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    pred = extract_final_number(raw)\n",
    "    return pred, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh8-xIYUMbgs",
    "outputId": "fd5f14b7-91a2-4128-ec90-03d090bb0380"
   },
   "outputs": [],
   "source": [
    "def evaluate_subset(data: List[Dict[str, Any]], n_samples: int = 20):\n",
    "    n_samples = min(n_samples, len(data))\n",
    "    total, correct = 0, 0\n",
    "    info = []\n",
    "\n",
    "    for idx in range(n_samples):\n",
    "        sample = data[idx]\n",
    "        gold = sample[\"qa\"][\"exe_ans\"]\n",
    "\n",
    "        selected = select_evidence(sample)\n",
    "        pred, raw_output = generate_answer(sample, selected)\n",
    "\n",
    "        is_ok = False\n",
    "        if pred is not None:\n",
    "            try:\n",
    "                if abs(pred - gold) <= 1e-2:\n",
    "                    is_ok = True\n",
    "            except TypeError:\n",
    "                is_ok = False\n",
    "\n",
    "        total += 1\n",
    "        if is_ok:\n",
    "            correct += 1\n",
    "\n",
    "        print(f\"[{idx}] pred={pred}, gold={gold}, correct={is_ok}\")\n",
    "        print(\"Selected evidence:\")\n",
    "        for s in selected:\n",
    "            print(\"  -\", \" \".join(s.split()))\n",
    "        print(\"RAW LLM:\", raw_output[:300], \"\\n\")\n",
    "\n",
    "        info.append({\n",
    "            \"idx\": idx,\n",
    "            \"pred\": pred,\n",
    "            \"gold\": gold,\n",
    "            \"selected\": selected,\n",
    "            \"raw\": raw_output,\n",
    "            \"correct\": is_ok,\n",
    "        })\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\nExecution Accuracy: {correct}/{total} = {acc:.4f}\")\n",
    "    return acc, info\n",
    "\n",
    "acc_dev, info_dev = evaluate_subset(dev_raw, n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bfvp-7JzMdM3",
    "outputId": "cf0414d1-34bf-41e3-ad7e-a7569d0bb57c"
   },
   "outputs": [],
   "source": [
    "acc_lora_50, info_lora_50 = evaluate_subset(dev_raw, n_samples=50)\n",
    "print(\"Finetuned LoRA on 50 dev examples, accuracy =\", acc_lora_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ddd22b12468b405a82b3dab98b0f048e",
      "afa575c1bacc4dd5a02d78013f914c10",
      "fecd47c7daf64a9dafe7a2be175d2a6b",
      "1688e5a07da940a49585cbcac697b499",
      "58d2911be0824f9ba92a90398cccbd3a",
      "d83f1bd1c2614d289e32227642967304",
      "4974142bed754bf0a16998338c3ff039",
      "d4c5ea0a2b69466aa5cc80f317b33120",
      "8cd3d750551a48e286359076c9d6af04",
      "6f125ab0393f4298aeee60e7a1f0f1c9",
      "507785f0c100491dbbb2f876a566f9d2"
     ]
    },
    "id": "owptFdB8O7qH",
    "outputId": "61893164-461a-4b3d-af97-144e87c1d689"
   },
   "outputs": [],
   "source": [
    "# Zero-shot baseline\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "zs_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_MATH,           # \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "zs_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "zs_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_zeroshot(sample: Dict[str, Any], selected_texts: List[str]):\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    raw = qwen7b_answer_generate(\n",
    "        question=q,\n",
    "        evidence_texts=selected_texts,\n",
    "        tokenizer=zs_tokenizer,\n",
    "        model=zs_model,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    pred = extract_final_number(raw)\n",
    "    return pred, raw\n",
    "\n",
    "\n",
    "def evaluate_subset_zeroshot(data: List[Dict[str, Any]], n_samples: int = 20):\n",
    "    n_samples = min(n_samples, len(data))\n",
    "    total, correct = 0, 0\n",
    "    info = []\n",
    "\n",
    "    for idx in range(n_samples):\n",
    "        sample = data[idx]\n",
    "        gold = sample[\"qa\"][\"exe_ans\"]\n",
    "\n",
    "        selected = select_evidence(sample)\n",
    "        pred, raw_output = generate_answer_zeroshot(sample, selected)\n",
    "\n",
    "        is_ok = False\n",
    "        if pred is not None:\n",
    "            try:\n",
    "                if abs(pred - gold) <= 1e-2:\n",
    "                    is_ok = True\n",
    "            except TypeError:\n",
    "                is_ok = False\n",
    "\n",
    "        total += 1\n",
    "        if is_ok:\n",
    "            correct += 1\n",
    "\n",
    "        print(f\"[ZS {idx}] pred={pred}, gold={gold}, correct={is_ok}\")\n",
    "        print(\"Selected evidence:\")\n",
    "        for s in selected:\n",
    "            print(\"  -\", \" \".join(s.split()))\n",
    "        print(\"RAW LLM (zero-shot):\", raw_output[:300], \"\\n\")\n",
    "\n",
    "        info.append({\n",
    "            \"idx\": idx,\n",
    "            \"pred\": pred,\n",
    "            \"gold\": gold,\n",
    "            \"selected\": selected,\n",
    "            \"raw\": raw_output,\n",
    "            \"correct\": is_ok,\n",
    "        })\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\n[Zero-shot] Execution Accuracy: {correct}/{total} = {acc:.4f}\")\n",
    "    return acc, info\n",
    "\n",
    "# Compare with previous 20's result\n",
    "acc_zs_20, info_zs_20 = evaluate_subset_zeroshot(dev_raw, n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4doBbEnQBgI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
