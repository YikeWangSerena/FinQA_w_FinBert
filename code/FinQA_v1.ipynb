{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrNaoR2sRLUh",
    "outputId": "c41ab49e-b3ed-4c78-c0ea-5debe28995b1"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate datasets\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/FinQA/dataset\"\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.json\")\n",
    "DEV_PATH = os.path.join(DATA_DIR, \"dev.json\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.json\")\n",
    "PRIVATE_TEST_PATH = os.path.join(DATA_DIR, \"private_test.json\")\n",
    "\n",
    "print(\"Check files:\")\n",
    "for p in [TRAIN_PATH, DEV_PATH, TEST_PATH, PRIVATE_TEST_PATH]:\n",
    "    print(p, os.path.exists(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWXnFGqYRQhT",
    "outputId": "11ea28fd-6d87-4013-8b35-07978a529a7d"
   },
   "outputs": [],
   "source": [
    "def load_json(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} samples from {path}\")\n",
    "    return data\n",
    "\n",
    "train_raw = load_json(TRAIN_PATH)\n",
    "dev_raw = load_json(DEV_PATH)\n",
    "test_raw = load_json(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avft-Dx9RR0l",
    "outputId": "e49562f6-0ed2-47bd-adcb-bd3a35f9ef5e"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def build_sft_record(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    mi = sample[\"qa\"][\"model_input\"]  # list of [id, text]\n",
    "\n",
    "    evidences = []\n",
    "    for i, (_id, text) in enumerate(mi):\n",
    "        t = \" \".join(text.split())\n",
    "        evidences.append(f\"[{i}] {t}\")\n",
    "    ev_block = \"\\n\".join(evidences)\n",
    "\n",
    "    gold = sample[\"qa\"][\"exe_ans\"]  # float\n",
    "\n",
    "    system = \"You are a financial numerical reasoning expert.\"\n",
    "    user = f\"\"\"Question:\n",
    "{q}\n",
    "\n",
    "Evidence:\n",
    "{ev_block}\n",
    "\n",
    "Please compute the final numeric answer.\n",
    "Always end your reply with:\n",
    "Final answer: <number>\"\"\"\n",
    "\n",
    "    assistant = f\"Final answer: {gold}\"\n",
    "\n",
    "    return {\n",
    "        \"system\": system,\n",
    "        \"user\": user,\n",
    "        \"assistant\": assistant,\n",
    "    }\n",
    "\n",
    "train_records = [build_sft_record(s) for s in train_raw]\n",
    "len(train_records), train_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527,
     "referenced_widgets": [
      "02dc9b070b3f4a0893863aefb58bf9e2",
      "932f2de8fd16490c8836942b6c66d317",
      "dd3334f1f56045fb9877d983c561601d",
      "e5d80bb6178044f0a6baca1308a195df",
      "6dd9e4fde4bd444584036e043ce413a9",
      "60386f0b5ddb4bdf96efd5cc6d7ec2d1",
      "9ed9b921f53046098e02b98584e6e330",
      "f1bc336f433045898a02dd461b47ff35",
      "a76ee415aab840c4ba113d94a1e0e34f",
      "912889c5a02448c28e5c4d12ae34351c",
      "14fcb0d2e3234b01ae39ca90d1272a5f",
      "68d81e3753714d97aff5079289856b39",
      "ffb98aa3174a4502b785657e43af48d2",
      "74038921e204443eb49227504b80011b",
      "47a9df8ad8694ce4bdffaade0958a789",
      "d0497cde07dc4d5483ef57160a7a2b45",
      "16525c099a4944fd9090f3408715e0b7",
      "804f0ed37d624bf88002104163b58b4d",
      "7fa68867b2ed415bb1587370a4dc0573",
      "b72c52edd5b14ac7a452fc657979415e",
      "dfcb0765c1664b429579b7b385f2ae54",
      "3d09e61f1b57419ca8971306b2a9392e",
      "c0790c79c5cf4c7e8016cae335ad8f25",
      "2b1febe284d740dbaea3604379d29df4",
      "f2e023258a6d4048ac2d4cfda9f343cc",
      "2e1db51f14544e73a633de36a83516aa",
      "93112abe8fbb40b58cdeb33e88e59829",
      "4c9e1fb4614a402b94fe3d3963ffae15",
      "fa39347b25af44e9821f35dc47620303",
      "855e8bb5e29f42a8a8a7230f464febc3",
      "59cd69fdddd444609aa524a93e68f330",
      "c9d9e49807fb4106bbc98bde7758b2d1",
      "bb6f835ca8904be785aa950073c75529",
      "0767f010e12d44699b9e4cceaf319a71",
      "66af7f4e95f64b56af8c8e2787c720c1",
      "acdefdd326c249cfb8bc4542a909fc7b",
      "31785887a2514c1ba52470df9c76c564",
      "7477783b1f87470d9f1346513850efa4",
      "f8e9a69574ff4aceb0d5d825770895ce",
      "c8258163bd3048b7b0a54232acac4ba3",
      "0a1d0ae3e7f94d91a22251460cdbdf2c",
      "07ed3d55924848148abee27ecb528899",
      "acb535af58304ed1b7204913c6cfe570",
      "5ee885726a4f48a49eb73835ade75481",
      "bef32f5af9b04e98ade405c453d0e496",
      "95b365129bef4d03ab9e25e2575c420a",
      "a2cf6b7e38214aaf9ac5e33bde07c1a3",
      "2bcbe6d3a30b47358b00712ace0a999b",
      "3eeccce8161840b99ebc134b0c6199cf",
      "4fcdb67ecbb343e0a1b0b9a248cf5a4c",
      "805398f6e9f143aea9a2012ef0e5ec5a",
      "6150b2aec92c41ef8ca454a2bf9d94a1",
      "0f4e593ef5694203a68345698b2d6d93",
      "6611a42c250a4024af581de9a68f2326",
      "f87e4436a2f04434a26cdd01e555fb3f"
     ]
    },
    "id": "N_l_1XpJRwNk",
    "outputId": "138fbcae-115a-428a-bbcb-919a8eee7c72"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME_MATH = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer_math = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_list(train_records)\n",
    "\n",
    "def add_text(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": example[\"user\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"assistant\"]},\n",
    "    ]\n",
    "    text = tokenizer_math.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_ds.map(add_text)\n",
    "print(train_ds[0][\"text\"][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357,
     "referenced_widgets": [
      "eb5df4f4a176409f8c1cb736f6e072dd",
      "3f85b1865051409aaec17569354a606e",
      "da9445db86254db9930a7cab41dfccd7",
      "ed714c8b0d344a1b80c4c88a59566833",
      "a33a83e8e64348b78c83f96b9fd7e03a",
      "dc9435fdf7884c1085619ef411bf1f9e",
      "a8caaa0ba8b04d0586519de65445f926",
      "2a432e0034b74b6893091cb0e2a0f12f",
      "81997eb8ecf7437e9e37fa3fa6e9733d",
      "5c4fef81afa64618b239505049726e05",
      "8ac7fa1da516452c8fff60c009e69aa2",
      "3f387390b6fa4bc9903248f7b2afeb07",
      "2d1172e152de4068addd58b3857166df",
      "a9f1c2b7d6664d328434682a05ff58e1",
      "c26b185a08f64b879fc4231faf1dff47",
      "99947bea5dee4bcf9b4cf9ee03da17e2",
      "f96d3aa8713d407e9a6283c5a6facfa4",
      "c9ddff99e2864d21814092e942d72269",
      "038ad6c7b29147d099fe8182ffaac36a",
      "76ce36734881445199cfd428db53e94f",
      "f6e8cc787b1d4be59a19ee653b034d8d",
      "7d0a460112c34b619bed61ff184d54ac",
      "5995cde56c2044b98b325cf28ad0039f",
      "f884aa5020494580ad9250322db8f373",
      "bd8f321c0269438fbcedbc421ec32e85",
      "e2ecd025c7824d2d9394977ede488c3b",
      "0b4cb144985040c488e778a5b98db40e",
      "cb45f52b044d41df8268b71d2f474deb",
      "f957fa64824a4fe2b60aca8e8f081546",
      "d4eb82a9cbdd48d9a677a1320f17b929",
      "889c03f37d47463b8529f3990ca42651",
      "286fb77fe94b43a2a2ba2da1d53595fe",
      "d4180268c1e149a0b8b65997579e4812",
      "23ee21c7d7f44cd68a44428d7d31bfc9",
      "5d0812248c0e48a3bd799d1e0e04d69c",
      "166a66824be34f7cbb8f1121eed04b9f",
      "94dfdf3f24544c899da04c3dda540973",
      "9b89a635997a4299a35daed36f78ab78",
      "b0be9a90ac1643f29a51b773a2c57004",
      "3502b991e0b6450caada4da77a349356",
      "760d68e892ae4f1b8b1c050c358c270f",
      "133d7faae436421a970c892d2ec0b197",
      "f1975fc0d3ee437fadeb5cb2d79301a7",
      "4af6bb17f538406e829987435a3387c1",
      "9a20725a06ad4bdbb580b64e0d252182",
      "feeeed8274144008aea725b191911943",
      "ad6f54783f184ccca6c8d0d2995e7c82",
      "6ea271a362af4666b682e0e84c756c94",
      "69b1289a7f1d43c0b7843e0b2ef35b2c",
      "dbba061baae14d1384f15d39a68a43f3",
      "e447669384964a58a38a11141a1ca5dc",
      "59a4a998ce8448f3846bf95e51cf50b3",
      "da973743ac064c409c8f75c299c11486",
      "6e9f6c31e5af417091476d7e738be5c6",
      "db980555fa674d71ab4c61b2bd39e394",
      "56016d89655b4c70852da70b273fabf5",
      "a65b30304a1d41019b8a7e4c44ef20d5",
      "7132408b1ecf473cb9d4829b585a1169",
      "c12a1d9456bc4d28a4bdb151714a7330",
      "5fe99be84c6f44e2b701c0fa48a02028",
      "d28dbe9c89ac45909fffdcfd4b19d76c",
      "7ad43a13962a4942a7b0520a7cc9dc52",
      "9c16bd2c63064898acf5eb7eeac31d37",
      "5a3c20572b4b4102992a93099f27e4bc",
      "56ec496dc5fd4a81ab83393ff4bf88de",
      "972a0acca8ca4bde87bd02ff359d2351",
      "2a27b2db5b8c4dceba6b74f8d1420791",
      "b1fa2eb929374e0abd50283a7ba591cd",
      "e3af06f51f3d49919c0ed87b4d8dd982",
      "58ebd8ebfc644da9bb283b4cf391e83c",
      "e134944f674a41c78761e7ee9b565966",
      "21676af3b5c846f7ba0493960b541eab",
      "57f0e7478f434888b42b07ef0c7f84ae",
      "22fa149ab6e740d3a1127639b992053d",
      "61ffe9e2bf7f4b619dd06903513fb986",
      "68b6d47eed9b467dadfbca4c09750192",
      "916af703eaaf4159aebbcfc2be18bc96",
      "d834cc16f1b94b97b4d224244a848318",
      "8e6774b3c9a041d4933d54d68026df54",
      "f20a0aa5a68a4a4d9dda704b6b61266c",
      "78e0b5da755a4c02a08e0402a4d310a8",
      "ba7c9cc18dec4269bb572e5654718e1e",
      "366012086f6345e3addf9d84abb049e0",
      "db36431955964ac08502349ecec1273e",
      "0960fcd61964450e82cbe36be110cff5",
      "a3bc3c29036b4df6aa87d61ceefbb1df",
      "dd58513f37e84419b7337e2ad0108c20",
      "e6a71ae67d3a4841a4a907d6ed9e67fc",
      "2e57ca1d909a48f18476a3eafdb0c52e",
      "8c149e47ae784295a0398e716e36160b",
      "b6165083ed844882b8d296044a219650",
      "cc0a1144ca6048cf95c0ba9872c5c357",
      "3bb2ec14c9a64f20940626b6ef54f23c",
      "ff4016a1f894456d80343b06a6ab6bf0",
      "582544b5438648dc92c01f0c367b9a43",
      "55db1fd091184998bf20fe0eeb68ad42",
      "b73ffe528a9a49839f1c7306a4749641",
      "ad916f9b03d349dfa4cc009383c7b82d",
      "d77f105a94ac4ded921dd70e7c498771"
     ]
    },
    "id": "Wcl_jUAxSFkL",
    "outputId": "4f2fd9b2-9e71-4b22-b4b8-84c84b4de93a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_math_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model_math_base.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model_math = get_peft_model(model_math_base, lora_config)\n",
    "model_math.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k6b8mur0SLoN",
    "outputId": "ccadb962-9187-4223-f2b3-f4bc46b7a2bf"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "MAX_LENGTH = 768\n",
    "\n",
    "def data_collator(batch):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    enc = tokenizer_math(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
    "    return enc\n",
    "\n",
    "train_ds_full = train_ds\n",
    "print(\"Train size:\", len(train_ds_full))\n",
    "\n",
    "OUTPUT_DIR = \"/content/qwen2p5_7b_finqa_lora_v2\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_math,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_full,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kyrLesoiSQJ8",
    "outputId": "72cd6c0c-9a0e-42fd-f413-4546419f4104"
   },
   "outputs": [],
   "source": [
    "LORA_SAVE_DIR = \"/content/qwen2p5_7b_finqa_lora_v2\"\n",
    "model_math.save_pretrained(LORA_SAVE_DIR)\n",
    "tokenizer_math.save_pretrained(LORA_SAVE_DIR)\n",
    "print(\"LoRA adapter saved to:\", LORA_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "4ca605dc940b41b59a6087659bf4272e",
      "281ed4ec702b4a34bd6de2a4fcc975bd",
      "e8a97befacad4967a6667918d7bb0a22",
      "6a579252aa7f4114ac36a0f11b6ca678",
      "421be5b08bfb4dce9b72831d11bf834d",
      "2cf3b317f63a417c90de198d9383fa7b",
      "86a333909d084a8fb59a81224d0436e4",
      "73751cf3b09d41dfbf89781b297109c4",
      "5abb4a8e08894dd1b35227965f01eb27",
      "f5b940c89d454af1a4eed9b0c5b322f0",
      "bff96512fce94fe0922093307820a4ca",
      "bc09bd21ccea45f5bd0e02436f5dffe9",
      "d36399d233554c2faa0aeb41a4d4f5d4",
      "6ef9b8dcf38c461d849e9a4e55376e9d",
      "c46e388875b14dcfa75dadc10c86b305",
      "33fce7ce4ac044d4be249263b596aebd",
      "5765696f7f56402ab39f1c6266a3bc5b",
      "b2421e6b646d4ba8934c1980e192d76f",
      "d18e8ed9a00c4e09ae4bd2558bb09f31",
      "a3dded8343b14700ba2864f7f0caff36",
      "bbca8247c16341f28e01375ad963d32f",
      "fae0f5eff15547c389403627991fb8f6"
     ]
    },
    "id": "9KFYmpIrSRxH",
    "outputId": "d7fc8f44-51f0-4818-a79d-d00eb1fea69f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "RETRIEVER_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    RETRIEVER_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "retriever_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RETRIEVER_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "retriever_model.eval()\n",
    "\n",
    "gen_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "gen_model = PeftModel.from_pretrained(gen_base_model, LORA_SAVE_DIR)\n",
    "gen_model.eval()\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LORA_SAVE_DIR,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLu45RplSSpa"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "@torch.no_grad()\n",
    "def retriever_generate(prompt: str, max_new_tokens: int = 64) -> str:\n",
    "    inputs = retriever_tokenizer(prompt, return_tensors=\"pt\").to(retriever_model.device)\n",
    "    out_ids = retriever_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=retriever_tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = retriever_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if out.startswith(prompt):\n",
    "        out = out[len(prompt):]\n",
    "    return out.strip()\n",
    "\n",
    "def build_retriever_prompt(question: str, candidates: List[str]) -> str:\n",
    "    cand_lines = []\n",
    "    for i, text in enumerate(candidates):\n",
    "        t = \" \".join(text.split())\n",
    "        cand_lines.append(f\"[{i}] {t}\")\n",
    "    cand_block = \"\\n\".join(cand_lines)\n",
    "    prompt = textwrap.dedent(\n",
    "        \"You help solve financial numerical reasoning questions.\\n\\n\"\n",
    "        \"Given a question and a list of candidate evidence texts, select ALL evidence\\n\"\n",
    "        \"sentences that are directly needed to compute the numeric answer.\\n\\n\"\n",
    "        \"Only output a comma-separated list of indices.\\n\"\n",
    "        \"Do NOT output anything else.\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        + question\n",
    "        + \"\\n\\nCandidate evidence:\\n\"\n",
    "        + cand_block\n",
    "        + \"\\n\\nAnswer (indices only):\"\n",
    "    ).strip()\n",
    "    return prompt\n",
    "\n",
    "def parse_indices(s: str, max_idx: int) -> List[int]:\n",
    "    nums = re.findall(r\"\\d+\", s)\n",
    "    ids = []\n",
    "    for t in nums:\n",
    "        i = int(t)\n",
    "        if 0 <= i < max_idx:\n",
    "            ids.append(i)\n",
    "    return sorted(set(ids))\n",
    "\n",
    "def select_evidence(sample: Dict[str, Any]) -> List[str]:\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    mi = sample[\"qa\"][\"model_input\"]\n",
    "    candidates = [t for (_id, t) in mi]\n",
    "    prompt = build_retriever_prompt(q, candidates)\n",
    "    raw = retriever_generate(prompt)\n",
    "    idxs = parse_indices(raw, len(candidates))\n",
    "    if not idxs:\n",
    "        idxs = list(range(min(5, len(candidates))))\n",
    "    selected = [candidates[i] for i in idxs]\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN99Y9fDSSj7"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def qwen7b_answer_generate(\n",
    "    question: str,\n",
    "    evidence_texts: List[str],\n",
    "    tokenizer,\n",
    "    model,\n",
    "    max_new_tokens: int = 128,\n",
    ") -> str:\n",
    "    ev_block = \"\\n\".join(\"Evidence: \" + \" \".join(e.split()) for e in evidence_texts)\n",
    "    system_prompt = (\n",
    "        \"You are a financial numerical reasoning expert. \"\n",
    "        \"Given a question and evidence sentences, you must:\\n\"\n",
    "        \"1) Identify exactly which numbers are needed.\\n\"\n",
    "        \"2) Write out the calculation step by step.\\n\"\n",
    "        \"3) Pay very careful attention to UNITS and SCALE.\\n\"\n",
    "        \"- If the question asks for a percentage, decide clearly whether the answer\\n\"\n",
    "        \"  should be a raw ratio (e.g., 0.12) or a percent (e.g., 12.0), and DO NOT\\n\"\n",
    "        \"  arbitrarily multiply or divide by 100.\\n\"\n",
    "        \"- Do NOT just copy a number from the evidence. Always perform the required\\n\"\n",
    "        \"  addition/subtraction/multiplication/division.\\n\"\n",
    "        \"You may show reasoning steps, but MUST end your response with exactly:\\n\"\n",
    "        \"Final answer: <number>\\n\"\n",
    "        \"Only output one final numeric value in the 'Final answer' line.\"\n",
    "    )\n",
    "    user_content = (\n",
    "        \"Question:\\n\"\n",
    "        + question\n",
    "        + \"\\n\\nEvidence:\\n\"\n",
    "        + ev_block\n",
    "        + \"\\n\\nPlease compute the final numeric answer.\\n\"\n",
    "        + \"Always end with:\\n\"\n",
    "        + \"Final answer: <number>\\n\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    chat_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    input_ids = chat_inputs\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    input_len = input_ids.shape[1]\n",
    "    out_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen_ids = out_ids[0, input_len:]\n",
    "    out = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return out.strip()\n",
    "\n",
    "def extract_final_number(output: str):\n",
    "    m = re.search(r\"Final answer\\s*:\\s*([-+]?\\d*\\.?\\d+)\", output, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", output)\n",
    "    if nums:\n",
    "        return float(nums[-1])\n",
    "    return None\n",
    "\n",
    "def generate_answer(sample: Dict[str, Any], selected_texts: List[str]):\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    raw = qwen7b_answer_generate(\n",
    "        question=q,\n",
    "        evidence_texts=selected_texts,\n",
    "        tokenizer=gen_tokenizer,\n",
    "        model=gen_model,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    pred = extract_final_number(raw)\n",
    "    return pred, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zSnhszcSSdW",
    "outputId": "61b775d2-5f28-48eb-c041-70220077c743"
   },
   "outputs": [],
   "source": [
    "def evaluate_subset(data: List[Dict[str, Any]], n_samples: int = 20):\n",
    "    n_samples = min(n_samples, len(data))\n",
    "    total, correct = 0, 0\n",
    "    info = []\n",
    "    for idx in range(n_samples):\n",
    "        sample = data[idx]\n",
    "        gold = sample[\"qa\"][\"exe_ans\"]\n",
    "        selected = select_evidence(sample)\n",
    "        pred, raw_output = generate_answer(sample, selected)\n",
    "        is_ok = False\n",
    "        if pred is not None:\n",
    "            try:\n",
    "                if abs(pred - gold) <= 1e-2:\n",
    "                    is_ok = True\n",
    "            except TypeError:\n",
    "                is_ok = False\n",
    "        total += 1\n",
    "        if is_ok:\n",
    "            correct += 1\n",
    "        print(f\"[{idx}] pred={pred}, gold={gold}, correct={is_ok}\")\n",
    "        print(\"Selected evidence:\")\n",
    "        for s in selected:\n",
    "            print(\"  -\", \" \".join(s.split()))\n",
    "        print(\"RAW LLM:\", raw_output[:400], \"\\n\")\n",
    "        info.append({\n",
    "            \"idx\": idx,\n",
    "            \"pred\": pred,\n",
    "            \"gold\": gold,\n",
    "            \"selected\": selected,\n",
    "            \"raw\": raw_output,\n",
    "            \"correct\": is_ok,\n",
    "        })\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\nExecution Accuracy: {correct}/{total} = {acc:.4f}\")\n",
    "    return acc, info\n",
    "\n",
    "acc_lora_20, info_lora_20 = evaluate_subset(dev_raw, n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8a08aa59c8944792b79f354bd5122561",
      "ce867773612348f1b81fb8df2022cfa1",
      "a3739808774a4a93a366c2b7cf7bcc66",
      "a8d069ca828b41b3a8c3ef0403a9839a",
      "a2bb76ed79a048fa9d680b0af41ae857",
      "8c930cb0a96a4e27a448e52e406474cc",
      "d665f841a4bd4b00adff7dd326e33c17",
      "a14835ea9fa74e2e9583cde9fbf74964",
      "5f87a86dfb8b4be2bc3bbcce2c71034a",
      "d18e62a251e4485ab20247ea2577df17",
      "c7e5f8862eb948749e66f93113522f0c"
     ]
    },
    "id": "cZHlsQaQSZOt",
    "outputId": "22ea3877-4dfd-4d4b-8420-ac139d7173c0"
   },
   "outputs": [],
   "source": [
    "zs_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "zs_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_MATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "zs_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_zeroshot(sample: Dict[str, Any], selected_texts: List[str]):\n",
    "    q = sample[\"qa\"][\"question\"]\n",
    "    raw = qwen7b_answer_generate(\n",
    "        question=q,\n",
    "        evidence_texts=selected_texts,\n",
    "        tokenizer=zs_tokenizer,\n",
    "        model=zs_model,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    pred = extract_final_number(raw)\n",
    "    return pred, raw\n",
    "\n",
    "def evaluate_subset_zeroshot(data: List[Dict[str, Any]], n_samples: int = 20):\n",
    "    n_samples = min(n_samples, len(data))\n",
    "    total, correct = 0, 0\n",
    "    info = []\n",
    "    for idx in range(n_samples):\n",
    "        sample = data[idx]\n",
    "        gold = sample[\"qa\"][\"exe_ans\"]\n",
    "        selected = select_evidence(sample)\n",
    "        pred, raw_output = generate_answer_zeroshot(sample, selected)\n",
    "        is_ok = False\n",
    "        if pred is not None:\n",
    "            try:\n",
    "                if abs(pred - gold) <= 1e-2:\n",
    "                    is_ok = True\n",
    "            except TypeError:\n",
    "                is_ok = False\n",
    "        total += 1\n",
    "        if is_ok:\n",
    "            correct += 1\n",
    "        print(f\"[ZS {idx}] pred={pred}, gold={gold}, correct={is_ok}\")\n",
    "        print(\"Selected evidence:\")\n",
    "        for s in selected:\n",
    "            print(\"  -\", \" \".join(s.split()))\n",
    "        print(\"RAW LLM (zero-shot):\", raw_output[:400], \"\\n\")\n",
    "        info.append({\n",
    "            \"idx\": idx,\n",
    "            \"pred\": pred,\n",
    "            \"gold\": gold,\n",
    "            \"selected\": selected,\n",
    "            \"raw\": raw_output,\n",
    "            \"correct\": is_ok,\n",
    "        })\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\n[Zero-shot] Execution Accuracy: {correct}/{total} = {acc:.4f}\")\n",
    "    return acc, info\n",
    "\n",
    "acc_zs_20, info_zs_20 = evaluate_subset_zeroshot(dev_raw, n_samples=20)\n",
    "print(\"Zero-shot acc_20 =\", acc_zs_20, \"Finetuned acc_20 =\", acc_lora_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SG_xwsuSSZIl",
    "outputId": "7d800dec-daa9-4122-fa51-02e24562f7c9"
   },
   "outputs": [],
   "source": [
    "acc_lora_50, info_lora_50 = evaluate_subset(dev_raw, n_samples=50)\n",
    "print(\"Finetuned LoRA on 50 dev examples, accuracy =\", acc_lora_50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
